{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 12. Implement Logistic Regression with Mini-batch Gradient Descent using TensorFlow\n",
    "Train it and evaluate it on the moons dataset (introduced in Chapter 5). Try\n",
    "adding all the bells and whistles:<br/>\n",
    "• Define the graph within a logistic_regression() function that can be reused\n",
    "easily.<br/>\n",
    "• Save checkpoints using a Saver at regular intervals during training, and save\n",
    "the final model at the end of training.<br/>\n",
    "• Restore the last checkpoint upon startup if training was interrupted.<br/>\n",
    "• Define the graph using nice scopes so the graph looks good in TensorBoard.<br/>\n",
    "• Add summaries to visualize the learning curves in TensorBoard.<br/>\n",
    "• Try tweaking some hyperparameters such as the learning rate or the mini-\n",
    "batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mkl\n",
    "mkl.get_max_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "rnd.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Instances 150 #Features 4\n"
     ]
    }
   ],
   "source": [
    "m,n = X.data.shape\n",
    "print('#Instances', m, '#Features', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 876.5,  458.1,  563.8,  179.8]),\n",
       " array([ -2.53547183e-13,  -2.45553577e-13,  -2.22377672e-13,\n",
       "         -2.43471909e-13]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum(axis=0), X_scaled.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression:\n",
    "\n",
    "Prediction\n",
    "p = sigma(W*x + b)\n",
    "sigma(z) = 1 / (1 + exp(-z))\n",
    "\n",
    "Loss\n",
    "L = -(1/m) * sum(y * log(p) + (1-y) * log(1-p))\n",
    "dL/dwj = (1/m) * sum(p - y) * xj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Base line solution (by Scikit-Learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "reg = LogisticRegression()\n",
    "reg.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.81016631,  1.39369878, -1.68738578, -1.51899135],\n",
       "        [ 0.13037985, -1.2463382 ,  0.78919477, -0.88943988],\n",
       "        [ 0.01299039, -0.1445346 ,  1.86317337,  2.69887272]]),\n",
       " array([-1.61861686, -0.89769778, -2.70851029]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_, reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 0.364803930181\n",
      "Epoch 5000 loss 0.17283139084\n",
      "Epoch 10000 loss 0.123850291703\n",
      "Epoch 15000 loss 0.099680961851\n",
      "Epoch 20000 loss 0.0848750721397\n",
      "Epoch 25000 loss 0.074726873854\n",
      "Epoch 30000 loss 0.067267056679\n",
      "Epoch 35000 loss 0.0615133565135\n",
      "Epoch 40000 loss 0.0569167312832\n",
      "Epoch 45000 loss 0.0531444755301\n",
      "Epoch 50000 loss 0.049982380163\n",
      "Epoch 55000 loss 0.0472858458247\n",
      "Epoch 60000 loss 0.0449534778041\n",
      "Epoch 65000 loss 0.0429119176552\n",
      "Epoch 70000 loss 0.0411066821754\n",
      "Epoch 75000 loss 0.0394963955113\n",
      "Epoch 80000 loss 0.0380490276877\n",
      "Epoch 85000 loss 0.0367393665913\n",
      "Epoch 90000 loss 0.0355472739815\n",
      "Epoch 95000 loss 0.0344564545427\n",
      "Epoch 100000 loss 0.0334535693248\n",
      "Final weights: [[ 0.75111156]\n",
      " [-2.64949059]\n",
      " [ 3.29147732]\n",
      " [ 1.86372929]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100001\n",
    "learning_rate = 0.001\n",
    "\n",
    "X_op = tf.placeholder(dtype=np.float64, shape=(None, n), name='X')\n",
    "y_op = tf.placeholder(dtype=np.float64, shape=(None, 1), name='y')\n",
    "W = tf.Variable(initial_value=np.random.randn(n, 1), name='W')\n",
    "b = tf.Variable(initial_value=np.zeros((1,1)), name='b')\n",
    "p = tf.sigmoid(tf.matmul(X_op, W) + b, name='prediction')\n",
    "# loss = tf.losses.log_loss(y_op, p, scope='loss')\n",
    "loss = tf.reduce_mean(-tf.log(p) * y_op)\n",
    "gradients = tf.gradients(loss, [W])[0]\n",
    "train_op = tf.assign(W, W - gradients * learning_rate)\n",
    "\n",
    "feed_dict = {X_op : X_scaled, y_op : y.reshape((-1, 1))}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "        if epoch % 5000 == 0:\n",
    "            print('Epoch', epoch, 'loss', sess.run(loss, feed_dict=feed_dict))\n",
    "    \n",
    "    print('Final weights:', sess.run(W))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
